---
title: 'MachineLearning Summary2'
layout: post
tags:
  - ML
category: MachineLearning
---
#### SUMMARY2
김성훈교수님 youtube 동영상 강의 요약, https://www.youtube.com/watch?v=n7DNueHGkqE

<br><br>

#### 1 regression

1.1 linear regression - Hypothesis, 가설, linear line 을 찾는 과정이 바로 학습이다.
* 가설을 세우고 실제 값과의 차이를 계산하는것 --> cost function
```
H(x) = wx + b
(H(x)-y)^2 ==> 음수를 양수화하고 차이가 많을 수록 패널티가 커짐
```
* minimize cost(W,b)  ==> cost 를 최소화하는 W, b를 찾는 과정 => 학습
==> 최소의 cost를 찾아가는 방법, gradient descent ==> 미분하여 경사도를 계산
* cost function을 설계할때 반드시 convex function(밥그릇을 엎어놓은것과 같은 모양)의 형태가 되도록해야 gradient descent가 제대로
동작함을 보장할 수 있다.

1.2 multi-variable linear regression
* Hypothesis
```
H(x1,x2,x3) = w1x1 + w2x2 + w3x3 + b
H(X) = XW  <==  Hypothesis using Matrix
```

<br><br>

##### 2 classification

1.1 Logistic classification
* linear regression 의 단점 보완필요.
* logistic hypothtesis, sigmoid, 0~1 로 정규화
```
z = WX, H(X) = g(z)
```
* cost function 이 local 영역에서 멈출 수 있음. global minimize 를 찾아야함. -> new cost function 이 필요.
* C(H(x),y) ==> log를 사용하여 non-linear한 cost function 을 개선
```
C(H(x),y) = -log(H(x))     :    y=1
                 = -log(1-H(x))   :    y=0
C(H(x),y) = -ylog(H(x)) - (1-y)log(1-H(x))
```

1.2 Softmax classification : Multinomial classification
* cost function, cross entropy - D(S,L)

<br><br>

#### 3 learning rate, overfitting, regularization

* learning rate : large learning rate: overshooting
* data preprocessing : data normalization 이 필요

<br><br>

#### 4 Deep Neural Nets

* XOR 문제에 봉착
* Activation Functions : sum(wi*xi + b)
* Rebranding to Deep Nets, Deep Learning

<br><br>

#### 5 Neural Network(NN)
* XOR 해결 방법
```
K(X) = sigmoid(XW1 + B1)
Y = H(X) = sigmoid(K(X)W2 + B2)
==>
K = tr.sigmoid(tf.matmul(X, W1) + b1)
hypothesis = tf.sigmoid(tf,matmul(K, W2) + b2)
```
* <B>Back propagation (chain rule)</B>
```
f=wx+b, g=wx, f=g+b
af/ax = af/ag * ag/ax
```