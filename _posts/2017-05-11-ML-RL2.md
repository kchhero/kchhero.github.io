---
categories: Uncategoried
category: MachineLearning
layout: post
tags:
  - ML
title: 'MachineLearning Reinforcement Learning2'
---
#### Reinforcement Learning2
강화학습관련 동영상 강의 요약
김성훈교수 youtube 동영상 강의 : https://www.youtube.com/watch?v=MQ-3QScrFSI&feature=youtu.be

<br><br>

#### Q-learning exploit&exploration and discounted reward
exploit : 현재 값을 이용..
exploration : 새로운 길을 찾아가는...
ex) 식당 : 주중에는 경험에 의한 맛집을 찾는다, 주말에는 새로운 맛집을 찾는다.

##### method1 : E-greedy
```python
e  = 0.1
if rand < e:
    a = random     #10% 확률로 새로운 길로..
else:
    a = argmax(Q(s,a))  #90% 확률로 아는 길로
```

<br>

##### method2 : decaying E-greedy
횟수를 거듭할 수록 e 값이 작아진다. 점차적으로 경험에 의한 선택에 비중을 높인다.
```python
for i in range(1000):
    e  = 0.1 / (i+1)
    if random(1) < e:
        a = random     #10% 확률로 새로운 길로..
    else:
        a = argmax(Q(s,a))  #90% 확률로 아는 길로
```

<br>

##### method3 : add random noise
a = argmax(Q(s,a) + random_values)
a = argmax([0.5 0.6 0.3 0.2 0.5] + [0.1 0.2 0.7 0.3 0.1])

횟수를 거듭할 수록 e 값이 작아진다. 점차적으로 경험에 의한 선택에 비중을 높인다.
완전한 random 은 아님, 2~3순위의 길에 비중을 높인다.
```python
for i in range(1000):
    a = argmax(Q(s,a) + random_values/(i+1))
```

<br>

##### discounted future reward
* ex) FrozenLake map의 길찾기에서... 최단거리로 가는 방법을 어떻게 찾을 것인가?
* idea : discounted reward
	Q(s,a) <-- r + g*maxQ(s',a')     ==>   g : gamma

* future reward : R = r1 + r2 + ... + r(n)
* future rewad : R(t) = r(t) + r(t+1) + ... + r(n)

* discounted furure reward : R(t) = r(t) + g*r(t+1) + g^2*r(t+2) + ... + g^(n-t)*r(n) = r(t) + g*R(t+1)
* Q(s,a) = r + g*maxQ(s',a')

<br>

##### 실습3 : E&E + discounted reward
실습2의 코드에서 몇군데만 수정하면 된다.
```python
def playgame() :
   (snip)...
    num_episodes = 2000
    dis = 0.99  # discount factor

    rList = []
    for i in range(num_episodes):
        (snip)...
        e = 1. / ((i / 100)+1)  # decaying E-greedy

        while not done:
            if np.random.rand(1) < e:    # decaying E-greedy
                action = env.action_space.sample()
            else:
                action = rargmax(Q[state, :])
                # add random noise
                #action = rargmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i+1))

            new_state, reward, done,_ = env.step(action)

            Q[state, action] = reward + dis*np.max(Q[new_state,:]) # discount reward

출력:
Success rate: 0.817
Final Q-Table Values
LEFT DOWN RIGHT UP
[[ 0.94148015  0.95099005  0.95099005  0.94148015]
 [ 0.94148015  0.          0.96059601  0.95099005]
 [ 0.95099005  0.970299    0.95099005  0.96059601]
 [ 0.96059601  0.          0.95099005  0.95099005]
 [ 0.95099005  0.96059601  0.          0.94148015]
 [ 0.          0.          0.          0.        ]
 [ 0.          0.9801      0.          0.96059601]
 [ 0.          0.          0.          0.        ]
 [ 0.96059601  0.          0.970299    0.95099005]
 [ 0.96059601  0.9801      0.9801      0.        ]
 [ 0.970299    0.99        0.          0.970299  ]
 [ 0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.        ]
 [ 0.          0.9801      0.99        0.970299  ]
 [ 0.9801      0.99        1.          0.9801    ]
 [ 0.          0.          0.          0.        ]]
```

